{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "convolutional-neural-networks",
      "graded_item_id": "OMdut",
      "launcher_item_id": "bbBOL"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "YOLO test notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/YOLO-test/blob/master/YOLO_test_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyksyLElG6jA",
        "colab_type": "text"
      },
      "source": [
        "This notebook implements a simple object detection model. This is greatly inspired on the link here: [Awesome blog post!](https://towardsdatascience.com/yolov2-to-detect-your-own-objects-soccer-ball-using-darkflow-a4f98d5ce5bf)\n",
        "\n",
        "So far this is the best tutorial I have found. Many people implement this for a given system (linux, ios...) or with a video feed. This is ok, but I want to focus on what I want to do, object detection. Next step is a mask/semantic segmentation and on a much longer time scale, caption generation.\n",
        "\n",
        "Another useful link, stanford course presentation about this topic: [Slides](http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvE8OnXqYwQD",
        "colab_type": "code",
        "outputId": "04ef5798-80c7-48fb-9752-e20e966daf18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "import shutil\n",
        "import glob\n",
        "import os # for chdir\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = [18, 12]\n",
        "import h5py\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from keras import layers, optimizers\n",
        "from keras.layers import Input, Dense, Conv2D, Activation, ZeroPadding2D, BatchNormalization, Flatten, Add\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.utils import layer_utils, plot_model, to_categorical\n",
        "\n",
        "from keras.callbacks import History, ModelCheckpoint\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "keras.backend.set_image_data_format('channels_last')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "\n",
        "print(\"Everything imported correctly\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything imported correctly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X6P6rDpR9WV",
        "colab_type": "text"
      },
      "source": [
        "Now mount the files and then clone the git of the keras implementantion of the yolo model. Then download the weights  of the original model. It takes about 7 minutes, careful with deleting them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u5YhAJHUOe-",
        "colab_type": "code",
        "outputId": "8cdafa31-201e-4fd3-dc64-38fb0c581df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#First remove the folder and everything it contains.\n",
        "\n",
        "# !rm -rf YOLO-test\n",
        "!rm -rf keras-yolo3\n",
        "\n",
        "#Now I mount the folder\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive/\")\n",
        "root_path = \"gdrive/My Drive/YOLO/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExHcB3PtYq-s",
        "colab_type": "code",
        "outputId": "2a8e8f38-dbdf-419e-d47c-490806183065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# !cd gdrive/My Drive/YOLO/\n",
        "!rm -rf keras-yolo3\n",
        "\n",
        "!git clone https://github.com/qqwweee/keras-yolo3.git\n",
        "!cd keras-yolo3\n",
        "\n",
        "# !wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-yolo3'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "Receiving objects:   0% (1/144)   \rReceiving objects:   1% (2/144)   \rReceiving objects:   2% (3/144)   \rReceiving objects:   3% (5/144)   \rReceiving objects:   4% (6/144)   \rReceiving objects:   5% (8/144)   \rReceiving objects:   6% (9/144)   \rReceiving objects:   7% (11/144)   \rReceiving objects:   8% (12/144)   \rReceiving objects:   9% (13/144)   \rReceiving objects:  10% (15/144)   \rReceiving objects:  11% (16/144)   \rReceiving objects:  12% (18/144)   \rReceiving objects:  13% (19/144)   \rReceiving objects:  14% (21/144)   \rReceiving objects:  15% (22/144)   \rReceiving objects:  16% (24/144)   \rReceiving objects:  17% (25/144)   \rReceiving objects:  18% (26/144)   \rReceiving objects:  19% (28/144)   \rReceiving objects:  20% (29/144)   \rReceiving objects:  21% (31/144)   \rReceiving objects:  22% (32/144)   \rReceiving objects:  23% (34/144)   \rremote: Total 144 (delta 0), reused 0 (delta 0), pack-reused 144\u001b[K\n",
            "Receiving objects:  24% (35/144)   \rReceiving objects:  25% (36/144)   \rReceiving objects:  26% (38/144)   \rReceiving objects:  27% (39/144)   \rReceiving objects:  28% (41/144)   \rReceiving objects:  29% (42/144)   \rReceiving objects:  30% (44/144)   \rReceiving objects:  31% (45/144)   \rReceiving objects:  32% (47/144)   \rReceiving objects:  33% (48/144)   \rReceiving objects:  34% (49/144)   \rReceiving objects:  35% (51/144)   \rReceiving objects:  36% (52/144)   \rReceiving objects:  37% (54/144)   \rReceiving objects:  38% (55/144)   \rReceiving objects:  39% (57/144)   \rReceiving objects:  40% (58/144)   \rReceiving objects:  41% (60/144)   \rReceiving objects:  42% (61/144)   \rReceiving objects:  43% (62/144)   \rReceiving objects:  44% (64/144)   \rReceiving objects:  45% (65/144)   \rReceiving objects:  46% (67/144)   \rReceiving objects:  47% (68/144)   \rReceiving objects:  48% (70/144)   \rReceiving objects:  49% (71/144)   \rReceiving objects:  50% (72/144)   \rReceiving objects:  51% (74/144)   \rReceiving objects:  52% (75/144)   \rReceiving objects:  53% (77/144)   \rReceiving objects:  54% (78/144)   \rReceiving objects:  55% (80/144)   \rReceiving objects:  56% (81/144)   \rReceiving objects:  57% (83/144)   \rReceiving objects:  58% (84/144)   \rReceiving objects:  59% (85/144)   \rReceiving objects:  60% (87/144)   \rReceiving objects:  61% (88/144)   \rReceiving objects:  62% (90/144)   \rReceiving objects:  63% (91/144)   \rReceiving objects:  64% (93/144)   \rReceiving objects:  65% (94/144)   \rReceiving objects:  66% (96/144)   \rReceiving objects:  67% (97/144)   \rReceiving objects:  68% (98/144)   \rReceiving objects:  69% (100/144)   \rReceiving objects:  70% (101/144)   \rReceiving objects:  71% (103/144)   \rReceiving objects:  72% (104/144)   \rReceiving objects:  73% (106/144)   \rReceiving objects:  74% (107/144)   \rReceiving objects:  75% (108/144)   \rReceiving objects:  76% (110/144)   \rReceiving objects:  77% (111/144)   \rReceiving objects:  78% (113/144)   \rReceiving objects:  79% (114/144)   \rReceiving objects:  80% (116/144)   \rReceiving objects:  81% (117/144)   \rReceiving objects:  82% (119/144)   \rReceiving objects:  83% (120/144)   \rReceiving objects:  84% (121/144)   \rReceiving objects:  85% (123/144)   \rReceiving objects:  86% (124/144)   \rReceiving objects:  87% (126/144)   \rReceiving objects:  88% (127/144)   \rReceiving objects:  89% (129/144)   \rReceiving objects:  90% (130/144)   \rReceiving objects:  91% (132/144)   \rReceiving objects:  92% (133/144)   \rReceiving objects:  93% (134/144)   \rReceiving objects:  94% (136/144)   \rReceiving objects:  95% (137/144)   \rReceiving objects:  96% (139/144)   \rReceiving objects:  97% (140/144)   \rReceiving objects:  98% (142/144)   \rReceiving objects:  99% (143/144)   \rReceiving objects: 100% (144/144)   \rReceiving objects: 100% (144/144), 150.95 KiB | 2.96 MiB/s, done.\n",
            "Resolving deltas:   0% (0/65)   \rResolving deltas:   1% (1/65)   \rResolving deltas:   3% (2/65)   \rResolving deltas:  12% (8/65)   \rResolving deltas:  13% (9/65)   \rResolving deltas:  15% (10/65)   \rResolving deltas:  29% (19/65)   \rResolving deltas:  35% (23/65)   \rResolving deltas:  40% (26/65)   \rResolving deltas:  41% (27/65)   \rResolving deltas:  43% (28/65)   \rResolving deltas:  49% (32/65)   \rResolving deltas:  63% (41/65)   \rResolving deltas:  66% (43/65)   \rResolving deltas:  67% (44/65)   \rResolving deltas:  78% (51/65)   \rResolving deltas:  80% (52/65)   \rResolving deltas:  86% (56/65)   \rResolving deltas:  90% (59/65)   \rResolving deltas:  93% (61/65)   \rResolving deltas:  98% (64/65)   \rResolving deltas: 100% (65/65)   \rResolving deltas: 100% (65/65), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7JhKTU2e6kR",
        "colab_type": "text"
      },
      "source": [
        "Check in which directory I am. It's been a trillion years since I last used the linux commands, I'll keep some comments for the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnoKiEO4p-xA",
        "colab_type": "code",
        "outputId": "add979f2-d5d0-429b-ec43-704ecc34e363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkewaIHFm-PX",
        "colab_type": "code",
        "outputId": "1998e980-6efc-4c14-d513-86c3598be0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "shutil.copy(\"/content/gdrive/My Drive/YOLO/yolov3_custom.cfg\", \"/content/keras-yolo3\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/keras-yolo3/yolov3_custom.cfg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv0UZTCOfNza",
        "colab_type": "text"
      },
      "source": [
        "Now that I am in the correct directory I need to convert the weights I downloaded from darknet to keras. It takes some time but it's provided in the github of the keras implementation.\n",
        "\n",
        "The syntax is: !python, then file you want to convert. The config file with the weights file, which will be saved at the location of the last argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN9kQxVRtDK5",
        "colab_type": "code",
        "outputId": "5584ff27-cce2-479e-91a5-cc62c0ea9790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lle-Or4ItPgL",
        "colab_type": "code",
        "outputId": "092b2747-557c-4f82-a66a-8aa765a51d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/keras-yolo3\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pimtiNMaJ1Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python convert.py \"yolov3_custom.cfg\" \"../yolov3.weights\" \"../yolo.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYpE13Bcfrv7",
        "colab_type": "text"
      },
      "source": [
        "Now I will load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXNEaivOFMKM",
        "colab_type": "text"
      },
      "source": [
        "Now I should make sure I am in the correct directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcJ6RVHVFPwj",
        "colab_type": "code",
        "outputId": "749874f1-5286-4233-e136-17a7d03a096b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15q1xzVxFVp0",
        "colab_type": "text"
      },
      "source": [
        "I'm not, so I change with !cd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3adJfPhGFYoE",
        "colab_type": "code",
        "outputId": "26031406-1c56-4af3-894f-eac8f6eed266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ih_vylMJaFL",
        "colab_type": "text"
      },
      "source": [
        "First I am going to load the train and test samples and then I start with the model.\n",
        "\n",
        "First is to call X and Y to the pics and annotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21kqsmXYKCG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pics = []\n",
        "picnames = glob.glob(\"/content/gdrive/My Drive/YOLO/images/*.png\")\n",
        "picnames.sort()\n",
        "\n",
        "# About 100 times faster than a loop, list comprehensions!! I already knew, but this is still incredible.\n",
        "\n",
        "X = [Image.open(img) for img in picnames]\n",
        "\n",
        "#Comprehension lists are very confusing, sotranslating into a for loop it's:\n",
        "\n",
        "# for name in picnames:\n",
        "#     img = Image.open(name)\n",
        "#     pics.append(img)\n",
        "\n",
        "\n",
        "annotnames = glob.glob(\"/content/gdrive/My Drive/YOLO/annotations/*.xml\")\n",
        "annotnames.sort()\n",
        "\n",
        "Y = [an for an in annotnames]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COPkBzYDYZM5",
        "colab_type": "text"
      },
      "source": [
        "Check that the lengths are ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBd72ePvO7Cc",
        "colab_type": "code",
        "outputId": "a8457a8c-e4cb-4a32-dae2-d682ed819b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# picnames\n",
        "len(X)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beiDwrXPYdmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkeZ6bS1l9VQ",
        "colab_type": "code",
        "outputId": "84eb5c9b-2a96-42ec-c0a1-0a38793e04c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "loaded_model = load_model(\"yolo.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7trKA1iFuOH",
        "colab_type": "text"
      },
      "source": [
        "And the model was not compiled, add the weights and compile manually. First let's plot it to check that everything looks correct, which it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jizRLfJ2HPvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(loaded_model, show_shapes = True, to_file = \"loaded_model.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvwQ0_6KIGN2",
        "colab_type": "code",
        "outputId": "28d9138b-8115-4fc3-bb1b-c806378d49a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# loaded_model.load_weights(\"yolov3.weights\")\n",
        "\n",
        "opti = optimizers.Adam(lr = 1e-3)\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath = \"best_weights.h5\", monitor = \"val_acc\", save_best_only = True)\n",
        "\n",
        "loaded_model.compile(loss = \"categorical_crossentropy\", optimizer = opti, metrics = [\"accuracy\"])\n",
        "hist = History()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Se8UVuXJPvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "loaded_model.fit(train_x, train_y, epochs = num_epochs, batch_size = 32, validation_data = (test_x, test_y), shuffle = True, callbacks = [hist, checkpointer])\n",
        "\n",
        "# loaded_model.load_weights(\"Resnet/best_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zewK3NVZNHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Maybe I use this later. Just in case I leave it here to avoid copying it later\n",
        "\n",
        "#Now read data\n",
        "\n",
        "# with h5py.File(\"Resnet/test_signs.h5\") as test:\n",
        "#   test_x_1 = np.array(test[\"test_set_x\"][:]) # your train set features\n",
        "#   test_y_1 = np.array(test[\"test_set_y\"][:]) # your train set labels\n",
        "#   classes = np.array(test[\"list_classes\"][:]) # the list of classes\n",
        "\n",
        "# with h5py.File(\"Resnet/train_signs.h5\") as train:\n",
        "#   train_x_1 = np.array(train[\"train_set_x\"][:]) # train set features\n",
        "#   train_y_1 = np.array(train[\"train_set_y\"][:]) # train set labels\n",
        "\n",
        "# print(\"Everything loaded and imported\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}